(gpu) C:\Users\Gold\Desktop\workspace>python training.py
Using TensorFlow backend.
All the Libraries Imported Successfully
Step-1
Step-2
Loading WideResNet
WARNING:tensorflow:From C:\Users\Gold\Anaconda3\envs\gpu\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From C:\Users\Gold\Anaconda3\envs\gpu\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2019-05-29 17:06:14.768258: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2019-05-29 17:06:15.750998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62
pciBusID: 0000:01:00.0
totalMemory: 4.00GiB freeMemory: 3.30GiB
2019-05-29 17:06:15.759038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-05-29 17:06:16.269514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-05-29 17:06:16.272674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0
2019-05-29 17:06:16.274551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N
2019-05-29 17:06:16.278560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3004 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
Model is getting ready for compiling
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 32, 32, 3)    0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 32, 32, 16)   432         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 32, 32, 128)  18432       activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 32, 32, 128)  512         conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 32, 32, 128)  0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 32, 32, 128)  147456      activation_2[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 32, 32, 128)  2048        activation_1[0][0]
__________________________________________________________________________________________________
add_1 (Add)                     (None, 32, 32, 128)  0           conv2d_3[0][0]
                                                                 conv2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 32, 32, 128)  512         add_1[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 32, 32, 128)  0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 32, 32, 128)  147456      activation_3[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 32, 32, 128)  512         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 32, 32, 128)  0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 32, 128)  147456      activation_4[0][0]
__________________________________________________________________________________________________
add_2 (Add)                     (None, 32, 32, 128)  0           conv2d_6[0][0]
                                                                 add_1[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 32, 32, 128)  512         add_2[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 32, 128)  0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 16, 16, 256)  294912      activation_5[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 16, 16, 256)  1024        conv2d_7[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 16, 16, 256)  0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 16, 16, 256)  589824      activation_6[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 16, 16, 256)  32768       activation_5[0][0]
__________________________________________________________________________________________________
add_3 (Add)                     (None, 16, 16, 256)  0           conv2d_8[0][0]
                                                                 conv2d_9[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 16, 16, 256)  1024        add_3[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 16, 16, 256)  0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 16, 16, 256)  589824      activation_7[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 16, 16, 256)  1024        conv2d_10[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 16, 16, 256)  0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 16, 16, 256)  589824      activation_8[0][0]
__________________________________________________________________________________________________
add_4 (Add)                     (None, 16, 16, 256)  0           conv2d_11[0][0]
                                                                 add_3[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 16, 16, 256)  1024        add_4[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 16, 16, 256)  0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 8, 8, 512)    1179648     activation_9[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 8, 8, 512)    2048        conv2d_12[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 8, 8, 512)    0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 8, 8, 512)    2359296     activation_10[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 8, 8, 512)    131072      activation_9[0][0]
__________________________________________________________________________________________________
add_5 (Add)                     (None, 8, 8, 512)    0           conv2d_13[0][0]
                                                                 conv2d_14[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 8, 8, 512)    2048        add_5[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 8, 8, 512)    0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 8, 8, 512)    2359296     activation_11[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 8, 8, 512)    2048        conv2d_15[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 8, 8, 512)    0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 8, 8, 512)    2359296     activation_12[0][0]
__________________________________________________________________________________________________
add_6 (Add)                     (None, 8, 8, 512)    0           conv2d_16[0][0]
                                                                 add_5[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 8, 8, 512)    2048        add_6[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 8, 8, 512)    0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 8, 8, 512)    0           activation_13[0][0]
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 32768)        0           average_pooling2d_1[0][0]
__________________________________________________________________________________________________
pred_gender (Dense)             (None, 2)            65536       flatten_1[0][0]
__________________________________________________________________________________________________
pred_age (Dense)                (None, 101)          3309568     flatten_1[0][0]
==================================================================================================
Total params: 14,338,544
Trainable params: 14,331,344
Non-trainable params: 7,200
__________________________________________________________________________________________________
WARNING:tensorflow:From C:\Users\Gold\Anaconda3\envs\gpu\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING:tensorflow:From C:\Users\Gold\Anaconda3\envs\gpu\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
Train on 154666 samples, validate on 17186 samples
Epoch 1/50
2019-05-29 17:06:29.837387: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library cublas64_100.dll locally
2019-05-29 17:06:35.626759: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.59GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2019-05-29 17:06:35.786301: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.59GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.0:50 - loss: 9.9097 - pred_gender_loss: 1.0228 - pred_age_loss: 4.9659 - pred_gender_acc: 0.5376 - 154666/154666 [==============================] - 669s 4ms/step - loss: 7.3527 - pred_gender_loss: 0.8332 - pred_age_loss: 4.4078 - pred_gender_acc: 0.6265 - pred_age_acc: 0.0327 - val_loss: 5.6474 - val_pred_gender_loss: 0.5741 - val_pred_age_loss: 3.9508 - val_pred_gender_acc: 0.7126 - val_pred_age_acc: 0.0394

Epoch 00001: val_loss improved from inf to 5.64744, saving model to C:\Users\Gold\Desktop\workspace\checkpoint/weights.01-5.65.hdf5
Epoch 2/50
154666/154666 [==============================] - 635s 4ms/step - loss: 5.1390 - pred_gender_loss: 0.5322 - pred_age_loss: 3.9578 - pred_gender_acc: 0.7417 - pred_age_acc: 0.0352 - val_loss: 4.8773 - val_pred_gender_loss: 0.5388 - val_pred_age_loss: 3.9513 - val_pred_gender_acc: 0.7289 - val_pred_age_acc: 0.0357

Epoch 00002: val_loss improved from 5.64744 to 4.87727, saving model to C:\Users\Gold\Desktop\workspace\checkpoint/weights.02-4.88.hdf5
Epoch 3/50
154666/154666 [==============================] - 635s 4ms/step - loss: 4.6043 - pred_gender_loss: 0.4069 - pred_age_loss: 3.9052 - pred_gender_acc: 0.8180 - pred_age_acc: 0.0370 - val_loss: 4.5210 - val_pred_gender_loss: 0.3697 - val_pred_age_loss: 3.9256 - val_pred_gender_acc: 0.8384 - val_pred_age_acc: 0.0328

Epoch 00003: val_loss improved from 4.87727 to 4.52096, saving model to C:\Users\Gold\Desktop\workspace\checkpoint/weights.03-4.52.hdf5
Epoch 4/50
154666/154666 [==============================] - 635s 4ms/step - loss: 4.4090 - pred_gender_loss: 0.3468 - pred_age_loss: 3.8475 - pred_gender_acc: 0.8518 - pred_age_acc: 0.0394 - val_loss: 4.8086 - val_pred_gender_loss: 0.4485 - val_pred_age_loss: 4.1505 - val_pred_gender_acc: 0.7909 - val_pred_age_acc: 0.0413

Epoch 00004: val_loss did not improve from 4.52096
Epoch 5/50
154666/154666 [==============================] - 634s 4ms/step - loss: 4.3128 - pred_gender_loss: 0.3240 - pred_age_loss: 3.7887 - pred_gender_acc: 0.8644 - pred_age_acc: 0.0430 - val_loss: 4.2834 - val_pred_gender_loss: 0.3216 - val_pred_age_loss: 3.7750 - val_pred_gender_acc: 0.8673 - val_pred_age_acc: 0.0439

Epoch 00005: val_loss improved from 4.52096 to 4.28340, saving model to C:\Users\Gold\Desktop\workspace\checkpoint/weights.05-4.28.hdf5
Epoch 6/50
154666/154666 [==============================] - 635s 4ms/step - loss: 4.2454 - pred_gender_loss: 0.3103 - pred_age_loss: 3.7453 - pred_gender_acc: 0.8707 - pred_age_acc: 0.0439 - val_loss: 4.2771 - val_pred_gender_loss: 0.3165 - val_pred_age_loss: 3.7826 - val_pred_gender_acc: 0.8718 - val_pred_age_acc: 0.0375

Epoch 00006: val_loss improved from 4.28340 to 4.27709, saving model to C:\Users\Gold\Desktop\workspace\checkpoint/weights.06-4.28.hdf5
Epoch 7/50
154666/154666 [==============================] - 636s 4ms/step - loss: 4.1869 - pred_gender_loss: 0.3008 - pred_age_loss: 3.7102 - pred_gender_acc: 0.8764 - pred_age_acc: 0.0453 - val_loss: 4.2775 - val_pred_gender_loss: 0.3041 - val_pred_age_loss: 3.7924 - val_pred_gender_acc: 0.8773 - val_pred_age_acc: 0.0392

Epoch 00007: val_loss did not improve from 4.27709
Epoch 8/50
154666/154666 [==============================] - 636s 4ms/step - loss: 4.1551 - pred_gender_loss: 0.2939 - pred_age_loss: 3.6872 - pred_gender_acc: 0.8802 - pred_age_acc: 0.0466 - val_loss: 4.2744 - val_pred_gender_loss: 0.3053 - val_pred_age_loss: 3.7946 - val_pred_gender_acc: 0.8757 - val_pred_age_acc: 0.0407

Epoch 00008: val_loss improved from 4.27709 to 4.27445, saving model to C:\Users\Gold\Desktop\workspace\checkpoint/weights.08-4.27.hdf5
Epoch 9/50
154666/154666 [==============================] - 636s 4ms/step - loss: 4.1238 - pred_gender_loss: 0.2894 - pred_age_loss: 3.6681 - pred_gender_acc: 0.8826 - pred_age_acc: 0.0481 - val_loss: 4.6052 - val_pred_gender_loss: 0.3574 - val_pred_age_loss: 4.0866 - val_pred_gender_acc: 0.8433 - val_pred_age_acc: 0.0285

Epoch 00009: val_loss did not improve from 4.27445
Epoch 10/50
154666/154666 [==============================] - 641s 4ms/step - loss: 4.0984 - pred_gender_loss: 0.2854 - pred_age_loss: 3.6541 - pred_gender_acc: 0.8844 - pred_age_acc: 0.0484 - val_loss: 4.1631 - val_pred_gender_loss: 0.3147 - val_pred_age_loss: 3.6946 - val_pred_gender_acc: 0.8652 - val_pred_age_acc: 0.0475

Epoch 00010: val_loss improved from 4.27445 to 4.16309, saving model to C:\Users\Gold\Desktop\workspace\checkpoint/weights.10-4.16.hdf5
Epoch 11/50
154666/154666 [==============================] - 643s 4ms/step - loss: 4.0772 - pred_gender_loss: 0.2816 - pred_age_loss: 3.6413 - pred_gender_acc: 0.8868 - pred_age_acc: 0.0494 - val_loss: 4.1780 - val_pred_gender_loss: 0.3198 - val_pred_age_loss: 3.7087 - val_pred_gender_acc: 0.8663 - val_pred_age_acc: 0.0478

Epoch 00011: val_loss did not improve from 4.16309
Epoch 12/50
154666/154666 [==============================] - 643s 4ms/step - loss: 4.0610 - pred_gender_loss: 0.2793 - pred_age_loss: 3.6312 - pred_gender_acc: 0.8873 - pred_age_acc: 0.0499 - val_loss: 4.1522 - val_pred_gender_loss: 0.2979 - val_pred_age_loss: 3.7071 - val_pred_gender_acc: 0.8786 - val_pred_age_acc: 0.0459

Epoch 00012: val_loss improved from 4.16309 to 4.15221, saving model to C:\Users\Gold\Desktop\workspace\checkpoint/weights.12-4.15.hdf5
Epoch 13/50
154666/154666 [==============================] - 643s 4ms/step - loss: 4.0464 - pred_gender_loss: 0.2771 - pred_age_loss: 3.6220 - pred_gender_acc: 0.8888 - pred_age_acc: 0.0509 - val_loss: 4.1767 - val_pred_gender_loss: 0.2952 - val_pred_age_loss: 3.7372 - val_pred_gender_acc: 0.8807 - val_pred_age_acc: 0.0493

Epoch 00013: val_loss did not improve from 4.15221
Epoch 14/50
154666/154666 [==============================] - 643s 4ms/step - loss: 3.9237 - pred_gender_loss: 0.2547 - pred_age_loss: 3.5337 - pred_gender_acc: 0.9001 - pred_age_acc: 0.0592 - val_loss: 3.9836 - val_pred_gender_loss: 0.2719 - val_pred_age_loss: 3.5826 - val_pred_gender_acc: 0.8940 - val_pred_age_acc: 0.0549

Epoch 00014: val_loss improved from 4.15221 to 3.98357, saving model to C:\Users\Gold\Desktop\workspace\checkpoint/weights.14-3.98.hdf5
Epoch 15/50
154666/154666 [==============================] - 643s 4ms/step - loss: 3.8729 - pred_gender_loss: 0.2470 - pred_age_loss: 3.4990 - pred_gender_acc: 0.9029 - pred_age_acc: 0.0612 - val_loss: 4.0085 - val_pred_gender_loss: 0.2779 - val_pred_age_loss: 3.6064 - val_pred_gender_acc: 0.8882 - val_pred_age_acc: 0.0563

Epoch 00015: val_loss did not improve from 3.98357
Epoch 16/50
154666/154666 [==============================] - 643s 4ms/step - loss: 3.8376 - pred_gender_loss: 0.2418 - pred_age_loss: 3.4721 - pred_gender_acc: 0.9046 - pred_age_acc: 0.0634 - val_loss: 4.0201 - val_pred_gender_loss: 0.2707 - val_pred_age_loss: 3.6270 - val_pred_gender_acc: 0.8906 - val_pred_age_acc: 0.0546

Epoch 00016: val_loss did not improve from 3.98357
Epoch 17/50
154666/154666 [==============================] - 652s 4ms/step - loss: 3.8067 - pred_gender_loss: 0.2377 - pred_age_loss: 3.4458 - pred_gender_acc: 0.9065 - pred_age_acc: 0.0648 - val_loss: 4.0323 - val_pred_gender_loss: 0.2682 - val_pred_age_loss: 3.6414 - val_pred_gender_acc: 0.8918 - val_pred_age_acc: 0.0535

Epoch 00017: val_loss did not improve from 3.98357
Epoch 18/50
154666/154666 [==============================] - 645s 4ms/step - loss: 3.7745 - pred_gender_loss: 0.2327 - pred_age_loss: 3.4176 - pred_gender_acc: 0.9083 - pred_age_acc: 0.0680 - val_loss: 4.0099 - val_pred_gender_loss: 0.2727 - val_pred_age_loss: 3.6126 - val_pred_gender_acc: 0.8914 - val_pred_age_acc: 0.0524

Epoch 00018: val_loss did not improve from 3.98357
Epoch 19/50
154666/154666 [==============================] - 645s 4ms/step - loss: 3.7434 - pred_gender_loss: 0.2289 - pred_age_loss: 3.3878 - pred_gender_acc: 0.9095 - pred_age_acc: 0.0695 - val_loss: 4.1197 - val_pred_gender_loss: 0.2753 - val_pred_age_loss: 3.7169 - val_pred_gender_acc: 0.8898 - val_pred_age_acc: 0.0525

Epoch 00019: val_loss did not improve from 3.98357
Epoch 20/50
154666/154666 [==============================] - 646s 4ms/step - loss: 3.7112 - pred_gender_loss: 0.2241 - pred_age_loss: 3.3571 - pred_gender_acc: 0.9102 - pred_age_acc: 0.0719 - val_loss: 4.0555 - val_pred_gender_loss: 0.2834 - val_pred_age_loss: 3.6409 - val_pred_gender_acc: 0.8856 - val_pred_age_acc: 0.0571

Epoch 00020: val_loss did not improve from 3.98357
Epoch 21/50
154666/154666 [==============================] - 645s 4ms/step - loss: 3.6783 - pred_gender_loss: 0.2201 - pred_age_loss: 3.3241 - pred_gender_acc: 0.9115 - pred_age_acc: 0.0744 - val_loss: 4.0908 - val_pred_gender_loss: 0.2881 - val_pred_age_loss: 3.6671 - val_pred_gender_acc: 0.8872 - val_pred_age_acc: 0.0560

Epoch 00021: val_loss did not improve from 3.98357
Epoch 22/50
154666/154666 [==============================] - 645s 4ms/step - loss: 3.6442 - pred_gender_loss: 0.2165 - pred_age_loss: 3.2892 - pred_gender_acc: 0.9121 - pred_age_acc: 0.0785 - val_loss: 4.1025 - val_pred_gender_loss: 0.2933 - val_pred_age_loss: 3.6692 - val_pred_gender_acc: 0.8823 - val_pred_age_acc: 0.0558

Epoch 00022: val_loss did not improve from 3.98357
Epoch 23/50
154666/154666 [==============================] - 645s 4ms/step - loss: 3.6120 - pred_gender_loss: 0.2133 - pred_age_loss: 3.2556 - pred_gender_acc: 0.9131 - pred_age_acc: 0.0822 - val_loss: 4.1245 - val_pred_gender_loss: 0.2881 - val_pred_age_loss: 3.6916 - val_pred_gender_acc: 0.8835 - val_pred_age_acc: 0.0538

Epoch 00023: val_loss did not improve from 3.98357
Epoch 24/50
154666/154666 [==============================] - 643s 4ms/step - loss: 3.5777 - pred_gender_loss: 0.2099 - pred_age_loss: 3.2196 - pred_gender_acc: 0.9140 - pred_age_acc: 0.0847 - val_loss: 4.2660 - val_pred_gender_loss: 0.2967 - val_pred_age_loss: 3.8192 - val_pred_gender_acc: 0.8797 - val_pred_age_acc: 0.0552

Epoch 00024: val_loss did not improve from 3.98357
Epoch 25/50
154666/154666 [==============================] - 642s 4ms/step - loss: 3.5470 - pred_gender_loss: 0.2069 - pred_age_loss: 3.1868 - pred_gender_acc: 0.9138 - pred_age_acc: 0.0891 - val_loss: 4.2436 - val_pred_gender_loss: 0.3001 - val_pred_age_loss: 3.7884 - val_pred_gender_acc: 0.8832 - val_pred_age_acc: 0.0550

Epoch 00025: val_loss did not improve from 3.98357
Epoch 26/50
154666/154666 [==============================] - 642s 4ms/step - loss: 3.3699 - pred_gender_loss: 0.1868 - pred_age_loss: 3.0277 - pred_gender_acc: 0.9207 - pred_age_acc: 0.1102 - val_loss: 4.3674 - val_pred_gender_loss: 0.3243 - val_pred_age_loss: 3.8875 - val_pred_gender_acc: 0.8782 - val_pred_age_acc: 0.0561

Epoch 00026: val_loss did not improve from 3.98357
Epoch 27/50
154666/154666 [==============================] - 642s 4ms/step - loss: 3.2970 - pred_gender_loss: 0.1810 - pred_age_loss: 2.9599 - pred_gender_acc: 0.9216 - pred_age_acc: 0.1183 - val_loss: 4.4831 - val_pred_gender_loss: 0.3382 - val_pred_age_loss: 3.9884 - val_pred_gender_acc: 0.8765 - val_pred_age_acc: 0.0547

Epoch 00027: val_loss did not improve from 3.98357
Epoch 28/50
154666/154666 [==============================] - 641s 4ms/step - loss: 3.2522 - pred_gender_loss: 0.1770 - pred_age_loss: 2.9181 - pred_gender_acc: 0.9224 - pred_age_acc: 0.1229 - val_loss: 4.5340 - val_pred_gender_loss: 0.3371 - val_pred_age_loss: 4.0395 - val_pred_gender_acc: 0.8734 - val_pred_age_acc: 0.0536

Epoch 00028: val_loss did not improve from 3.98357
Epoch 29/50
154666/154666 [==============================] - 641s 4ms/step - loss: 3.2143 - pred_gender_loss: 0.1741 - pred_age_loss: 2.8822 - pred_gender_acc: 0.9231 - pred_age_acc: 0.1278 - val_loss: 4.6204 - val_pred_gender_loss: 0.3507 - val_pred_age_loss: 4.1113 - val_pred_gender_acc: 0.8745 - val_pred_age_acc: 0.0525

Epoch 00029: val_loss did not improve from 3.98357
Epoch 30/50
154666/154666 [==============================] - 641s 4ms/step - loss: 3.1803 - pred_gender_loss: 0.1722 - pred_age_loss: 2.8491 - pred_gender_acc: 0.9233 - pred_age_acc: 0.1320 - val_loss: 4.7056 - val_pred_gender_loss: 0.3535 - val_pred_age_loss: 4.1926 - val_pred_gender_acc: 0.8714 - val_pred_age_acc: 0.0539

Epoch 00030: val_loss did not improve from 3.98357
Epoch 31/50
154666/154666 [==============================] - 641s 4ms/step - loss: 3.1471 - pred_gender_loss: 0.1697 - pred_age_loss: 2.8173 - pred_gender_acc: 0.9233 - pred_age_acc: 0.1370 - val_loss: 4.7891 - val_pred_gender_loss: 0.3596 - val_pred_age_loss: 4.2690 - val_pred_gender_acc: 0.8698 - val_pred_age_acc: 0.0548

Epoch 00031: val_loss did not improve from 3.98357
Epoch 32/50
154666/154666 [==============================] - 641s 4ms/step - loss: 3.1156 - pred_gender_loss: 0.1683 - pred_age_loss: 2.7861 - pred_gender_acc: 0.9235 - pred_age_acc: 0.1414 - val_loss: 4.8523 - val_pred_gender_loss: 0.3597 - val_pred_age_loss: 4.3310 - val_pred_gender_acc: 0.8672 - val_pred_age_acc: 0.0509

Epoch 00032: val_loss did not improve from 3.98357
Epoch 33/50
154666/154666 [==============================] - 641s 4ms/step - loss: 3.0878 - pred_gender_loss: 0.1662 - pred_age_loss: 2.7593 - pred_gender_acc: 0.9240 - pred_age_acc: 0.1459 - val_loss: 4.9391 - val_pred_gender_loss: 0.3658 - val_pred_age_loss: 4.4106 - val_pred_gender_acc: 0.8655 - val_pred_age_acc: 0.0534

Epoch 00033: val_loss did not improve from 3.98357
Epoch 34/50
154666/154666 [==============================] - 641s 4ms/step - loss: 3.0603 - pred_gender_loss: 0.1644 - pred_age_loss: 2.7326 - pred_gender_acc: 0.9245 - pred_age_acc: 0.1503 - val_loss: 5.0793 - val_pred_gender_loss: 0.3794 - val_pred_age_loss: 4.5361 - val_pred_gender_acc: 0.8665 - val_pred_age_acc: 0.0519

Epoch 00034: val_loss did not improve from 3.98357
Epoch 35/50
154666/154666 [==============================] - 641s 4ms/step - loss: 3.0312 - pred_gender_loss: 0.1630 - pred_age_loss: 2.7038 - pred_gender_acc: 0.9246 - pred_age_acc: 0.1559 - val_loss: 5.2326 - val_pred_gender_loss: 0.4060 - val_pred_age_loss: 4.6616 - val_pred_gender_acc: 0.8688 - val_pred_age_acc: 0.0517

Epoch 00035: val_loss did not improve from 3.98357
Epoch 36/50
154666/154666 [==============================] - 642s 4ms/step - loss: 3.0065 - pred_gender_loss: 0.1622 - pred_age_loss: 2.6786 - pred_gender_acc: 0.9242 - pred_age_acc: 0.1609 - val_loss: 5.2549 - val_pred_gender_loss: 0.3959 - val_pred_age_loss: 4.6930 - val_pred_gender_acc: 0.8662 - val_pred_age_acc: 0.0500

Epoch 00036: val_loss did not improve from 3.98357
Epoch 37/50
154666/154666 [==============================] - 643s 4ms/step - loss: 2.9808 - pred_gender_loss: 0.1603 - pred_age_loss: 2.6537 - pred_gender_acc: 0.9249 - pred_age_acc: 0.1651 - val_loss: 5.3438 - val_pred_gender_loss: 0.4052 - val_pred_age_loss: 4.7714 - val_pred_gender_acc: 0.8658 - val_pred_age_acc: 0.0525

Epoch 00037: val_loss did not improve from 3.98357
Epoch 38/50
154666/154666 [==============================] - 643s 4ms/step - loss: 2.9535 - pred_gender_loss: 0.1593 - pred_age_loss: 2.6264 - pred_gender_acc: 0.9245 - pred_age_acc: 0.1687 - val_loss: 5.4081 - val_pred_gender_loss: 0.4126 - val_pred_age_loss: 4.8272 - val_pred_gender_acc: 0.8658 - val_pred_age_acc: 0.0527

Epoch 00038: val_loss did not improve from 3.98357
Epoch 39/50
154666/154666 [==============================] - 643s 4ms/step - loss: 2.8628 - pred_gender_loss: 0.1503 - pred_age_loss: 2.5441 - pred_gender_acc: 0.9291 - pred_age_acc: 0.1898 - val_loss: 5.4493 - val_pred_gender_loss: 0.4172 - val_pred_age_loss: 4.8635 - val_pred_gender_acc: 0.8636 - val_pred_age_acc: 0.0511

Epoch 00039: val_loss did not improve from 3.98357
Epoch 40/50
154666/154666 [==============================] - 643s 4ms/step - loss: 2.8449 - pred_gender_loss: 0.1493 - pred_age_loss: 2.5268 - pred_gender_acc: 0.9288 - pred_age_acc: 0.1951 - val_loss: 5.4713 - val_pred_gender_loss: 0.4189 - val_pred_age_loss: 4.8834 - val_pred_gender_acc: 0.8617 - val_pred_age_acc: 0.0513

Epoch 00040: val_loss did not improve from 3.98357
Epoch 41/50
154666/154666 [==============================] - 643s 4ms/step - loss: 2.8335 - pred_gender_loss: 0.1487 - pred_age_loss: 2.5157 - pred_gender_acc: 0.9288 - pred_age_acc: 0.1974 - val_loss: 5.5101 - val_pred_gender_loss: 0.4225 - val_pred_age_loss: 4.9183 - val_pred_gender_acc: 0.8613 - val_pred_age_acc: 0.0514

Epoch 00041: val_loss did not improve from 3.98357
Epoch 42/50
154666/154666 [==============================] - 644s 4ms/step - loss: 2.8264 - pred_gender_loss: 0.1482 - pred_age_loss: 2.5087 - pred_gender_acc: 0.9292 - pred_age_acc: 0.1984 - val_loss: 5.5464 - val_pred_gender_loss: 0.4236 - val_pred_age_loss: 4.9532 - val_pred_gender_acc: 0.8618 - val_pred_age_acc: 0.0514

Epoch 00042: val_loss did not improve from 3.98357
Epoch 43/50
154666/154666 [==============================] - 644s 4ms/step - loss: 2.8177 - pred_gender_loss: 0.1478 - pred_age_loss: 2.5001 - pred_gender_acc: 0.9286 - pred_age_acc: 0.2004 - val_loss: 5.5703 - val_pred_gender_loss: 0.4240 - val_pred_age_loss: 4.9764 - val_pred_gender_acc: 0.8598 - val_pred_age_acc: 0.0508

Epoch 00043: val_loss did not improve from 3.98357
Epoch 44/50
154666/154666 [==============================] - 645s 4ms/step - loss: 2.8094 - pred_gender_loss: 0.1471 - pred_age_loss: 2.4922 - pred_gender_acc: 0.9287 - pred_age_acc: 0.2013 - val_loss: 5.6232 - val_pred_gender_loss: 0.4267 - val_pred_age_loss: 5.0262 - val_pred_gender_acc: 0.8592 - val_pred_age_acc: 0.0488

Epoch 00044: val_loss did not improve from 3.98357
Epoch 45/50
154666/154666 [==============================] - 644s 4ms/step - loss: 2.8019 - pred_gender_loss: 0.1470 - pred_age_loss: 2.4844 - pred_gender_acc: 0.9293 - pred_age_acc: 0.2031 - val_loss: 5.6662 - val_pred_gender_loss: 0.4305 - val_pred_age_loss: 5.0651 - val_pred_gender_acc: 0.8592 - val_pred_age_acc: 0.0510

Epoch 00045: val_loss did not improve from 3.98357
Epoch 46/50
154666/154666 [==============================] - 644s 4ms/step - loss: 2.7947 - pred_gender_loss: 0.1465 - pred_age_loss: 2.4774 - pred_gender_acc: 0.9294 - pred_age_acc: 0.2052 - val_loss: 5.6663 - val_pred_gender_loss: 0.4303 - val_pred_age_loss: 5.0651 - val_pred_gender_acc: 0.8587 - val_pred_age_acc: 0.0508

Epoch 00046: val_loss did not improve from 3.98357
Epoch 47/50
154666/154666 [==============================] - 652s 4ms/step - loss: 2.7867 - pred_gender_loss: 0.1467 - pred_age_loss: 2.4688 - pred_gender_acc: 0.9290 - pred_age_acc: 0.2064 - val_loss: 5.7275 - val_pred_gender_loss: 0.4397 - val_pred_age_loss: 5.1166 - val_pred_gender_acc: 0.8602 - val_pred_age_acc: 0.0496

Epoch 00047: val_loss did not improve from 3.98357
Epoch 48/50
154666/154666 [==============================] - 1660s 11ms/step - loss: 2.7805 - pred_gender_loss: 0.1458 - pred_age_loss: 2.4632 - pred_gender_acc: 0.9292 - pred_age_acc: 0.2083 - val_loss: 5.7573 - val_pred_gender_loss: 0.4441 - val_pred_age_loss: 5.1416 - val_pred_gender_acc: 0.8594 - val_pred_age_acc: 0.0505

Epoch 00048: val_loss did not improve from 3.98357
Epoch 49/50
154666/154666 [==============================] - 646s 4ms/step - loss: 2.7721 - pred_gender_loss: 0.1451 - pred_age_loss: 2.4552 - pred_gender_acc: 0.9294 - pred_age_acc: 0.2094 - val_loss: 5.7465 - val_pred_gender_loss: 0.4387 - val_pred_age_loss: 5.1358 - val_pred_gender_acc: 0.8587 - val_pred_age_acc: 0.0488

Epoch 00049: val_loss did not improve from 3.98357
Epoch 50/50
154666/154666 [==============================] - 647s 4ms/step - loss: 2.7666 - pred_gender_loss: 0.1448 - pred_age_loss: 2.4497 - pred_gender_acc: 0.9294 - pred_age_acc: 0.2108 - val_loss: 5.8236 - val_pred_gender_loss: 0.4466 - val_pred_age_loss: 5.2048 - val_pred_gender_acc: 0.8588 - val_pred_age_acc: 0.0497

Model Weights Successfully saved.