Microsoft Windows [Version 10.0.18875.1000](c) 2019 Microsoft Corporation. All rights reserved.

C:\Users\Gold\Desktop\workspace>conda activate gpu

(gpu) C:\Users\Gold\Desktop\workspace>python training.py
Using TensorFlow backend.All the Libraries Imported Successfully
Step-1
Step-2
Loading WideResNet
WARNING:tensorflow:From C:\Users\Gold\Anaconda3\envs\gpu\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops
Instructions for updating:Colocations handled automatically by placer.
WARNING:tensorflow:From C:\Users\Gold\Anaconda3\envs\gpu\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.opsInstructions for updating:
Colocations handled automatically by placer.2019-05-30 18:34:17.984514: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2019-05-30 18:34:19.091518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62       
pciBusID: 0000:01:00.0
totalMemory: 4.00GiB freeMemory: 3.30GiB
2019-05-30 18:34:19.105648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-05-30 18:34:22.109602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-05-30 18:34:22.118026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0
2019-05-30 18:34:22.122529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N
2019-05-30 18:34:22.127308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3004 MB memory)i bus id: 0000:01:00.0, compute capability: 6.1)
Model is getting ready for compiling
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 32, 32, 3)    0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 32, 32, 16)   432         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 32, 32, 128)  18432       activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 32, 32, 128)  512         conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 32, 32, 128)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 32, 32, 128)  147456      activation_2[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 32, 32, 128)  2048        activation_1[0][0]
__________________________________________________________________________________________________
add_1 (Add)                     (None, 32, 32, 128)  0           conv2d_3[0][0]
                                                                 conv2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 32, 32, 128)  512         add_1[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 32, 32, 128)  0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 32, 32, 128)  147456      activation_3[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 32, 32, 128)  512         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 32, 32, 128)  0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 32, 128)  147456      activation_4[0][0]
__________________________________________________________________________________________________
add_2 (Add)                     (None, 32, 32, 128)  0           conv2d_6[0][0]
                                                                 add_1[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 32, 32, 128)  512         add_2[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 32, 128)  0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 16, 16, 256)  294912      activation_5[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 16, 16, 256)  1024        conv2d_7[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 16, 16, 256)  0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 16, 16, 256)  589824      activation_6[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 16, 16, 256)  32768       activation_5[0][0]
__________________________________________________________________________________________________
add_3 (Add)                     (None, 16, 16, 256)  0           conv2d_8[0][0]
                                                                 conv2d_9[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 16, 16, 256)  1024        add_3[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 16, 16, 256)  0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 16, 16, 256)  589824      activation_7[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 16, 16, 256)  1024        conv2d_10[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 16, 16, 256)  0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 16, 16, 256)  589824      activation_8[0][0]
__________________________________________________________________________________________________
add_4 (Add)                     (None, 16, 16, 256)  0           conv2d_11[0][0]
                                                                 add_3[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 16, 16, 256)  1024        add_4[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 16, 16, 256)  0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 8, 8, 512)    1179648     activation_9[0][0]
__________________________________________________________________________________________________
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 8, 8, 512)    0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 8, 8, 512)    2359296     activation_10[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 8, 8, 512)    131072      activation_9[0][0]
__________________________________________________________________________________________________
add_5 (Add)                     (None, 8, 8, 512)    0           conv2d_13[0][0]
                                                                 conv2d_14[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 8, 8, 512)    2048        add_5[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 8, 8, 512)    0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 8, 8, 512)    2359296     activation_11[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 8, 8, 512)    2048        conv2d_15[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 8, 8, 512)    0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 8, 8, 512)    2359296     activation_12[0][0]
__________________________________________________________________________________________________
add_6 (Add)                     (None, 8, 8, 512)    0           conv2d_16[0][0]
                                                                 add_5[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 8, 8, 512)    2048        add_6[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 8, 8, 512)    0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 8, 8, 512)    0           activation_13[0][0]
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 32768)        0           average_pooling2d_1[0][0]
__________________________________________________________________________________________________
pred_gender (Dense)             (None, 2)            65536       flatten_1[0][0]
__________________________________________________________________________________________________
pred_age (Dense)                (None, 101)          3309568     flatten_1[0][0]
==================================================================================================
Total params: 14,338,544
Trainable params: 14,331,344
Non-trainable params: 7,200
__________________________________________________________________________________________________
WARNING:tensorflow:From C:\Users\Gold\Anaconda3\envs\gpu\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated a
Instructions for updating:
Use tf.cast instead.
WARNING:tensorflow:From C:\Users\Gold\Anaconda3\envs\gpu\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated a
Instructions for updating:
Use tf.cast instead.
Train on 154666 samples, validate on 17186 samples
Epoch 1/30
2019-05-30 18:34:36.648196: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library cublas64_100.dll locally
2019-05-30 18:34:39.277479: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.59GiB. The caller indicates that rformance gains if more memory were available.
2019-05-30 18:34:39.464312: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.59GiB. The caller indicates that rformance gains if more memory were available.
154666/154666 [==============================] - 615s 4ms/step - loss: 6.9660 - pred_gender_loss: 0.7622 - pred_age_loss: 4.3126 - pred_gender_acc: 0.6585 - pred_age_acc: 0.0333red_age_loss: 4.0723 - val_pred_gender_acc: 0.7116 - val_pred_age_acc: 0.0365

Epoch 00001: val_loss improved from inf to 5.68872, saving model to C:\Users\Gold\Desktop\workspace\checkpoint/weights.01-5.69.hdf5
Epoch 2/30
154666/154666 [==============================] - 604s 4ms/step - loss: 5.1101 - pred_gender_loss: 0.5304 - pred_age_loss: 3.9925 - pred_gender_acc: 0.7498 - pred_age_acc: 0.0353red_age_loss: 4.0856 - val_pred_gender_acc: 0.7919 - val_pred_age_acc: 0.0351

Epoch 00002: val_loss improved from 5.68872 to 4.91727, saving model to C:\Users\Gold\Desktop\workspace\checkpoint/weights.02-4.92.hdf5
Epoch 3/30
154666/154666 [==============================] - 603s 4ms/step - loss: 4.7177 - pred_gender_loss: 0.4194 - pred_age_loss: 3.9355 - pred_gender_acc: 0.8134 - pred_age_acc: 0.0371pred_age_loss: 9.6983 - val_pred_gender_acc: 0.5844 - val_pred_age_acc: 0.0141

Epoch 00003: val_loss did not improve from 4.91727
Epoch 4/30
154666/154666 [==============================] - 602s 4ms/step - loss: 4.7128 - pred_gender_loss: 0.3950 - pred_age_loss: 3.9145 - pred_gender_acc: 0.8254 - pred_age_acc: 0.0371red_age_loss: 3.8913 - val_pred_gender_acc: 0.8238 - val_pred_age_acc: 0.0361

Epoch 00004: val_loss improved from 4.91727 to 4.50100, saving model to C:\Users\Gold\Desktop\workspace\checkpoint/weights.04-4.50.hdf5
Epoch 5/30
154666/154666 [==============================] - 602s 4ms/step - loss: 4.4158 - pred_gender_loss: 0.3490 - pred_age_loss: 3.8547 - pred_gender_acc: 0.8510 - pred_age_acc: 0.0391red_age_loss: 3.8724 - val_pred_gender_acc: 0.8302 - val_pred_age_acc: 0.0393

Epoch 00005: val_loss improved from 4.50100 to 4.43787, saving model to C:\Users\Gold\Desktop\workspace\checkpoint/weights.05-4.44.hdf5
Epoch 6/30
154666/154666 [==============================] - 601s 4ms/step - loss: 4.3161 - pred_gender_loss: 0.3276 - pred_age_loss: 3.7935 - pred_gender_acc: 0.8615 - pred_age_acc: 0.0419red_age_loss: 3.8379 - val_pred_gender_acc: 0.8574 - val_pred_age_acc: 0.0386

Epoch 00006: val_loss improved from 4.43787 to 4.38705, saving model to C:\Users\Gold\Desktop\workspace\checkpoint/weights.06-4.39.hdf5
Epoch 7/30
154666/154666 [==============================] - 602s 4ms/step - loss: 4.2304 - pred_gender_loss: 0.3138 - pred_age_loss: 3.7480 - pred_gender_acc: 0.8699 - pred_age_acc: 0.0442red_age_loss: 3.8665 - val_pred_gender_acc: 0.8500 - val_pred_age_acc: 0.0359

Epoch 00007: val_loss did not improve from 4.38705
Epoch 8/30
154666/154666 [==============================] - 601s 4ms/step - loss: 4.1738 - pred_gender_loss: 0.3035 - pred_age_loss: 3.7189 - pred_gender_acc: 0.8749 - pred_age_acc: 0.0455red_age_loss: 3.8229 - val_pred_gender_acc: 0.8506 - val_pred_age_acc: 0.0410

Epoch 00008: val_loss improved from 4.38705 to 4.31293, saving model to C:\Users\Gold\Desktop\workspace\checkpoint/weights.08-4.31.hdf5
Epoch 9/30
154666/154666 [==============================] - 601s 4ms/step - loss: 4.0283 - pred_gender_loss: 0.2755 - pred_age_loss: 3.6299 - pred_gender_acc: 0.8896 - pred_age_acc: 0.0520red_age_loss: 3.6493 - val_pred_gender_acc: 0.8848 - val_pred_age_acc: 0.0507

Epoch 00009: val_loss improved from 4.31293 to 4.04558, saving model to C:\Users\Gold\Desktop\workspace\checkpoint/weights.09-4.05.hdf5
Epoch 10/30
154666/154666 [==============================] - 602s 4ms/step - loss: 3.9793 - pred_gender_loss: 0.2678 - pred_age_loss: 3.5990 - pred_gender_acc: 0.8933 - pred_age_acc: 0.0539red_age_loss: 3.6507 - val_pred_gender_acc: 0.8886 - val_pred_age_acc: 0.0511

Epoch 00010: val_loss improved from 4.04558 to 4.03798, saving model to C:\Users\Gold\Desktop\workspace\checkpoint/weights.10-4.04.hdf5
Epoch 11/30
154666/154666 [==============================] - 602s 4ms/step - loss: 3.9479 - pred_gender_loss: 0.2623 - pred_age_loss: 3.5756 - pred_gender_acc: 0.8957 - pred_age_acc: 0.0553red_age_loss: 3.6339 - val_pred_gender_acc: 0.8869 - val_pred_age_acc: 0.0520

Epoch 00011: val_loss improved from 4.03798 to 4.01989, saving model to C:\Users\Gold\Desktop\workspace\checkpoint/weights.11-4.02.hdf5
Epoch 12/30
154666/154666 [==============================] - 603s 4ms/step - loss: 3.9187 - pred_gender_loss: 0.2568 - pred_age_loss: 3.5521 - pred_gender_acc: 0.8979 - pred_age_acc: 0.0585red_age_loss: 3.6500 - val_pred_gender_acc: 0.8861 - val_pred_age_acc: 0.0518

Epoch 00012: val_loss did not improve from 4.01989
Epoch 13/30
154666/154666 [==============================] - 607s 4ms/step - loss: 3.8913 - pred_gender_loss: 0.2521 - pred_age_loss: 3.5279 - pred_gender_acc: 0.8996 - pred_age_acc: 0.0594pred_age_loss: 3.6041 - val_pred_gender_acc: 0.8841 - val_pred_age_acc: 0.0545

Epoch 00014: val_loss improved from 4.01989 to 4.00523, saving model to C:\Users\Gold\Desktop\workspace\checkpoint/weights.14-4.01.hdf5
Epoch 15/30
154666/154666 [==============================] - 608s 4ms/step - loss: 3.8361 - pred_gender_loss: 0.2420 - pred_age_loss: 3.4767 - pred_gender_acc: 0.9036 - pred_age_acc: 0.0639pred_age_loss: 3.6648 - val_pred_gender_acc: 0.8789 - val_pred_age_acc: 0.0602

Epoch 00018: val_loss did not improve from 4.00523
Epoch 19/30
154666/154666 [==============================] - 608s 4ms/step - loss: 3.5544 - pred_gender_loss: 0.2009 - pred_age_loss: 3.2323 - pred_gender_acc: 0.9175 - pred_age_acc: 0.0878red_age_loss: 3.7492 - val_pred_gender_acc: 0.8733 - val_pred_age_acc: 0.0555

Epoch 00019: val_loss did not improve from 4.00523
Epoch 20/30
154666/154666 [==============================] - 608s 4ms/step - loss: 3.4830 - pred_gender_loss: 0.1938 - pred_age_loss: 3.1657 - pred_gender_acc: 0.9187 - pred_age_acc: 0.0942red_age_loss: 3.8330 - val_pred_gender_acc: 0.8715 - val_pred_age_acc: 0.0534

Epoch 00020: val_loss did not improve from 4.00523
Epoch 21/30
154666/154666 [==============================] - 608s 4ms/step - loss: 3.4001 - pred_gender_loss: 0.1859 - pred_age_loss: 3.0879 - pred_gender_acc: 0.9197 - pred_age_acc: 0.1040Epoch 00021: val_loss did not improve from 4.00523
Epoch 22/30
154666/154666 [==============================] - 607s 4ms/step - loss: 3.3103 - pred_gender_loss: 0.1789 - pred_age_loss: 3.0023 - pred_gender_acc: 0.9209 - pred_age_acc: 0.1175 - val_loss: 4.4493 - val_pred_gender_loss: 00.3528 - val_pred_age_loss: 3.9661 - val_pred_gender_acc: 0.8616 - val_pred_age_acc: 0.0550
Epoch 00022: val_loss did not improve from 4.00523
Epoch 23/30
154666/154666 [==============================] - 676s 4ms/step - loss: 3.2194 - pred_gender_loss: 0.1718 - pred_age_loss: 2.9154 - pred_gender_acc: 0.9217 - pred_age_acc: 0.1283 - val_loss: 4.5780 - val_pred_gender_loss: 00.3669 - val_pred_age_loss: 4.0776 - val_pred_gender_acc: 0.8620 - val_pred_age_acc: 0.0534
Epoch 00023: val_loss did not improve from 4.00523
83 - val_loss: 4.5780 - val_pred_gender_loss: 0.3669 - val_pred_age_loss: 4.0776 - val_pred_gender_acc: 0.8620 - val_pred_age_acc: 0.0534
                                                                                                                                                                               51 - val_loss: 4.6738 - val_pred_gender_loss: 0Epoch 00023: val_loss did not improve from 4.00523
Epoch 24/30154666/154666 [==============================] - 641s 4ms/step - loss: 3.0237 - pred_gender_loss: 0.1503 - pred_age_loss: 2.7395 - pred_gender_acc: 0.9304 - pred_ag
e_acc: 0.16
51 - val_loss: 4.6738 - val_pred_gender_loss: 0.3890 - val_pred_age_loss: 4.1505 - val_pred_gender_acc: 0.8554 - val_pred_age_acc: 0.0555                                      82 - val_loss: 4.7702 - val_pred_gender_loss: 0
Epoch 00024: val_loss did not improve from 4.00523Epoch 25/30
154666/154666 [==============================] - 715s 5ms/step - loss: 2.9618 - pred_gender_loss: 0.1456 - pred_age_loss: 2.6814 - pred_gender_acc: 0.9307 - pred_ag
e_acc: 0.1766 [==============================] - 643s 4ms/step - loss: 2.9186 - pred_gender_loss: 0.1424 - pred_age_loss: 2.6404 - pred_gender_acc: 0.9309 - pred_age_acc: 0.1870 - val_loss: 4.8679 - val_pred_gender_loss: 0154666/154666 [==============================] - 641s 4ms/step - loss: 3.0237 - pred_gender_loss: 0.1503 - pred_age_loss: 2.7395 - pred_gender_acc: 0.9304 - pred_age_acc: 0.1651 - val_loss: 4.6738 - val_pred_gender_loss: 0.3890 - val_pred_age_loss: 4.1505 - val_pred_gender_acc: 0.8554 - val_pred_age_acc: 0.0555
Epoch 00024: val_loss did not improve from 4.00523
Epoch 25/30                                                                                                                                                         e_acc: 0.1960 - val_loss: 4.9484 - val_pred_gender_loss: 0154666/154666 [==============================] - 715s 5ms/step - loss: 2.9618 - pred_gender_loss: 0.1456 - pred_age_loss: 2.6814 - pred_gender_acc: 0.9307 - pred_age_acc: 0.1782 - val_loss: 4.7702 - val_pred_gender_loss: 0.4005 - val_pred_age_loss: 4.2345 - val_pred_gender_acc: 0.8542 - val_pred_age_acc: 0.0541
Epoch 00025: val_loss did not improve from 4.00523
Epoch 26/30                                                                                                                                                                    666 [>.............................] - ETA: 9:3154666/154666 [==============================] - 643s 4ms/step - loss: 2.9186 - pred_gender_loss: 0.1424 - pred_age_loss: 2.6404 - pred_gender_acc: 0.9309 - pred_age_acc: 0.2042 - val_loss: 5.0955 - val_pred_gender_loss: e_acc: 0.1870 - val_loss: 4.8679 - val_pred_gender_loss: 0.4118 - val_pred_age_loss: 4.3199 - val_pred_gender_acc: 0.8529 - val_pred_age_acc: 0.0553

Epoch 00026: val_loss did not improve from 4.00523
Epoch 27/30
154666/154666 [==============================] - 608s 4ms/step - loss: 2.8795 - pred_gender_loss: 0.1402 - pred_age_loss: 2.6026 - pred_gender_acc: 0.9309 - pred_ag           33 - val_loss: 5.1291 - val_pred_gender_loss:  80896/154666 [==============>...............] - ETA: 5:17 - loss: 2.7670 - pred_gender_loss: 0.1322 - pred_age_loss: 2.4952 - pred_gender_acc: 0.9324 - pred_age_ac 81024/154666 [==============>...............] - ETA: 5:16 - loss: 2.7672 - pred_gender_loss: 0.1323 - pred_age_loss: 2.4954 - pred_gender_acc: 0.9324 - pred_age_ac 81152/154666 [==============>...............] - ETA: 5:15 - loss: 2.7672 - pred_gender_loss: 0.1323 - pred_age_loss: 2.4954 - pred_gender_acc: 0.9324 - pred_age_ac 81280/154666 [==============>...............] - ETA: 5:15 - loss: 2.7672 - pred_gender_loss: 0.1324 - pred_age_loss: 2.4953 - pred_gender_acc: 0.9324 - pred_age_ac 81408/154666 [==============>...............] - 
ETA: 5:14 - loss: 2.7671 - pred_gender_loss: 0.1323 - pred_age_loss: 2.4953 - pred_gender_acc: 0.9325 - pred_age_ac 81536/154666 [==============>...............] - ETA: 5:14 - loss: 2.7671 - pred_gender_loss: 0.1323 - pred_age_loss: 2.4953 - pred_gender_acc: 0.9325 - pred_age_ac 81664/154666 [==============>...............] - ETA: 5:13 - loss: 2.7672 - pred_gender_loss: 0.1323 - pred_age_loss: 2.4953 - pred_gender_acc: 0.9325 - pred_age_ac 81792/154666 [==============>...............] - ETA: 5:12 - loss: 2.7671 - pred_gender_loss: 0.1323 - pred_age_loss: 2.4952 - pred_gender_acc: 0.9325 - pred_age_ac 81920/154666 [==============>...............] - ETA: 5:12 - loss: 2.7672 - pred_gender_loss: 0.1324 - pred_age_loss: 2.4952 - pred_gender_acc: 0.9325 - pred_age_ac 82048/154666 [==============>...............] - ETA: 5:11 - loss: 2.7676 - pred_gender_loss: 0.1324 - pred_age_loss: 2.4956 - pred_gender_acc: 0.9325 - pred_age_ac 82176/154666 [==============>...............] - ETA: 5:11 - loss: 2.7675 - pred_gender_loss: 0.1324 - pred_age_loss: 2.4955 - pred_gender_acc: 0.9325 - pred_age_ac 82304/154666 [==============>...............] - ETA: 5:10 - loss: 2.7675 - 
pred_gender_loss: 0.1325 - pred_age_loss: 2.4955 - pred_gender_acc: 0.9324 - pred_age_ac 82432/154666 [==============>...............] - ETA: 5:09 - loss: 2.7675 - pred_gender_loss: 0.1325 - pred_age_loss: 2.4955 - pred_gender_acc: 0.9324 - pred_age_ac 82560/154666 [===============>..............] - ETA: 5:09 - loss: 2.7675 - pred_gender_loss: 0.1325 - pred_age_loss: 2.4955 - pred_gender_acc: 0.9324 - pred_age_ac 82688/154666 [===============>..............] - ETA: 
5:08 - loss: 2.7671 - pred_gender_loss: 0.1324 - pred_age_loss: 2.4952 - pred_gender_acc: 0.9325 - pred_age_ac 82816/154666 [===============>..............] - ETA: 5:07 - loss: 2.7672 - pred_gender_loss: 0.1324 - pred_age_loss: 2.4952 - pred_gender_acc: 0.9325 - pred_age_ac 82944/154666 [===============>..............] - ETA: 5:07 - loss: 2.7671 - pred_gender_loss: 0.1324 - pred_age_loss: 2.4952 - pred_gender_acc: 0.9325 - pred_age_ac 83072/154666 [===============>..............] - ETA: 5:06 - loss: 2.7670 - pred_gender_loss: 0.1323 - pred_age_loss: 2.4951 - pred_gender_acc: 0.9325 - pred_age_ac 83200/154666 [===============>..............] - ETA: 5:06 - loss: 2.7668 - pred_gender_loss: 0.1323 - 
pred_age_loss: 2.4950 - pred_gender_acc: 0.9326 - pred_age_ac 83328/154666 [===============>..............] - ETA: 5:05 - loss: 2.7669 - pred_gender_loss: 0.1324 - pred_age_loss: 2.4950 - pred_gender_acc: 0.9325 - pred_age_ac 83456/154666 [===============>..............] - ETA: 5:04 - loss: 2.7674 - pred_gender_loss: 0.1324 - pred_age_loss: 2.4954 - pred_gender_acc: 0.9325 - pred_age_ac 83584/154666 [===============>..............] - ETA: 5:04 - loss: 2.7676 - pred_gender_loss: 0.1325 - pred_age_loss: 2.4955 - pred_gender_acc: 0.9324 - pred_age_ac 83712/154666 [===============>..............] - ETA: 5:03 - loss: 2.7675 - pred_gender_loss: 0.1326 - pred_age_loss: 2.4954 - pred_gender_acc: 0.9323 - pred_age_ac 83840/154666 [===============>..............] - ETA: 5:03 - loss: 2.7677 - pred_gender_loss: 0.1326 - pred_age_loss: 2.4955 - pred_gender_acc: 0.9323 - pred_age_ac 83968/154666 [===============>..............] - ETA: 5:02 - loss: 2.7676 - pred_gender_loss: 0.1326 - pred_age_loss: 2.4954 - pred_gender_acc: 0.9323 - pred_age_ac 84096/154666 [===============>..............] - ETA: 5:01 - loss: 2.7677 - pred_gender_loss: 0.1326 - pred_age_loss: 2.4955 - pred_gender_acc: 0.9323 - pred_age_ac 84224/154666 [===============>..............] - ETA: 5:01 - loss: 2.7680 - pred_gender_loss: 0.1327 - pred_age_loss: 2.4957 - pred_gender_acc: 0.9323 - pred_age_ac 84352/154666 [===============>..............] - ETA: 5:00 - loss: 2.7681 - pred_gender_loss: 0.1328 - pred_age_loss: 2.4958 - pred_gender_acc: 0.9322 - pred_age_ac 84480/154666 [===============>..............] - ETA: 5:00 - loss: 2.7682 - pred_gender_loss: 0.1329 - pred_age_loss: 2.4958 - pred_gender_acc: 0.9322 - pred_age_ac 84608/154666 [===============>..............] - ETA: 4:59 - loss: 2.7683 - pred_gender_loss: 0.1329 - pred_age_loss: 2.4958 - pred_gender_acc: 0.9321 - pred_age_ac 84736/154666 [===============>..............] - ETA: 4:59 - loss: 2.7683 - pred_gender_loss: 0.1330 - pred_age_loss: 2.4957 - pred_gender_acc: 0.9321 - pred_age_ac 84864/154666 [===============>..............] - ETA: 4:58 - loss: 2.7683 - pred_gender_loss: 0.1329 - pred_age_loss: 2.4957 - pred_gender_acc: 0.9321 - pred_age_ac 84992/154666 [===============>..............] - ETA: 4:57 - loss: 2.7684 - pred_gender_loss: 0.1330 - pred_age_loss: 2.4959 - pred_gender_acc: 0.9321 - pred154666/154666 [==============================] - 648s 4ms/step - loss: 2.7782 - pred_gender_loss: 0.1345 - pred_age_loss: 2.5039 - pred_gender_acc: 0.9311 - pred_age_acc: 0.2221 - val_loss: 5.2248 - val_pred_gender_loss: 0.4423 - val_pred_age_loss: 4.6422 - val_pred_gender_acc: 0.8478 - val_pred_age_acc: 0.0540Epoch 00030: val_loss did not improve from 4.00523First Trail SuccessfulTraceback (most recent call last):  File "C:\Users\Gold\Anaconda3\envs\gpu\lib\site-packages\pandas\io\pytables.py", line 466, in __init__    import tables  # noqaModuleNotFoundError: No module named 'tables'During handling of the above exception, another exception occurred:Traceback (most recent call last):  File "training.py", line 107, in <module>    pd.DataFrame(hist.history).to_hdf(output_path.joinpath("history_{}_{}.h5".format(depth, k)), "history")  File "C:\Users\Gold\Anaconda3\envs\gpu\lib\site-packages\pandas\core\generic.py", line 2377, in to_hdf    return pytables.to_hdf(path_or_buf, key, self, **kwargs)  File "C:\Users\Gold\Anaconda3\envs\gpu\lib\site-packages\pandas\io\pytables.py", line 273, in to_hdf    complib=complib) as store:  File "C:\Users\Gold\Anaconda3\envs\gpu\lib\site-packages\pandas\io\pytables.py", line 469, in __init__    'importing'.format(ex=ex))ImportError: HDFStore requires PyTables, "No module named 'tables'" problem importing(gpu) C:\Users\Gold\Desktop\workspace>(gpu) C:\Users\Gold\Desktop\workspace>(gpu) C:\Users\Gold\Desktop\workspace>pip install tablesCollecting tables  Downloading https://files.pythonhosted.org/packages/c3/3c/46ad5be02841d068ba0ce63c7549a7e17c838a938ca4c8a698ca23c2f467/tables-3.5.1-cp37-cp37m-win_amd64.whl (3.2MB)
     |████████████████████████████████| 3.2MB 1.6MB/s
Requirement already satisfied: mock>=2.0 in c:\users\gold\anaconda3\envs\gpu\lib\site-packages (from tables) (3.0.5)
Collecting numexpr>=2.6.2 (from tables)
  Downloading https://files.pythonhosted.org/packages/3d/ea/2da288c443310107f55ffaaf6afce6f7906692b00ccb7b787d0ba230f3f4/numexpr-2.6.9-cp37-none-win_amd64.whl (91kB)
     |████████████████████████████████| 92kB 5.8MB/s
Requirement already satisfied: numpy>=1.9.3 in c:\users\gold\anaconda3\envs\gpu\lib\site-packages (from tables) (1.16.4)
Requirement already satisfied: six>=1.9.0 in c:\users\gold\appdata\roaming\python\python37\site-packages (from tables) (1.12.0)
Installing collected packages: numexpr, tables
Successfully installed numexpr-2.6.9 tables-3.5.1

(gpu) C:\Users\Gold\Desktop\workspace>hist.history['loss']
'hist.history['loss']' is not recognized as an internal or external command,
operable program or batch file.

(gpu) C:\Users\Gold\Desktop\workspace>